# LLM連携モジュール概要

## 1. 目的と概要

LLM連携モジュールは、LLM学術文書支援システムの中核となるコンポーネントで、大規模言語モデルを活用して学術文書の理解と学習を支援します。このモジュールは以下の主要機能を提供します：

1. 複数のLLM APIとの統合インターフェース
2. 文脈理解のためのプロンプト生成と最適化
3. 長文コンテキスト管理と効率的な送信
4. 専門分野特化型のプロンプトテンプレート
5. ストリーミングレスポンスの処理

## 2. アーキテクチャ概要

```
┌─────────────────────────────────────────────────────────┐
│                  LLM連携モジュール                       │
│                                                         │
│  ┌───────────────┐  ┌───────────────┐  ┌───────────────┐│
│  │ LLMプロバイダー │  │プロンプト生成器 │  │コンテキスト  ││
│  │  アダプター    │  │               │  │  マネージャー  ││
│  └───────┬───────┘  └───────┬───────┘  └───────┬───────┘│
│          │                  │                  │        │
│          ▼                  ▼                  ▼        │
│  ┌───────────────┐  ┌───────────────┐  ┌───────────────┐│
│  │   API接続     │  │テンプレート管理│  │ レスポンス    ││
│  │  マネージャー  │  │               │  │ プロセッサー  ││
│  └───────────────┘  └───────────────┘  └───────────────┘│
└─────────────────────────────────────────────────────────┘
            │                                  ▲
            │                                  │
            ▼                                  │
┌─────────────────────────┐       ┌─────────────────────────┐
│                         │       │                         │
│     LLM API サービス    │       │   アプリケーション層    │
│  (Claude, GPT-4, etc.)  │       │                         │
└─────────────────────────┘       └─────────────────────────┘
```

## 3. 主要コンポーネント

### 3.1 LLMプロバイダーアダプター

複数のLLM APIに対する統一インターフェースを提供し、それぞれの特性に合わせた最適な通信を行います。

**主な責務**:
- 各LLM APIへの接続管理
- APIキーと認証の安全な管理
- 共通操作のための統一インターフェース提供
- 各モデルの特性に合わせたパラメータ調整

**サポートされるLLM**:
- Anthropic Claude API (Claude 3 Opus, Sonnet, Haiku)
- OpenAI API (GPT-4, GPT-3.5)
- 将来的に拡張予定のモデル

### 3.2 プロンプト生成器

ユーザーの質問と文書コンテキストから最適なプロンプトを生成します。

**主な責務**:
- 文脈を考慮したプロンプトの構築
- 専門分野に特化したプロンプトテンプレートの適用
- 効果的な指示と制約の設定
- プロンプトの長さと関連性の最適化

### 3.3 コンテキストマネージャー

文書内容を効率的にLLMのコンテキストウィンドウに収めるための管理を行います。

**主な責務**:
- コンテキストウィンドウサイズの管理
- 長文の分割と適切なチャンク化
- 関連コンテンツの選択と優先順位付け
- トークン数の計算と最適化

### 3.4 API接続マネージャー

LLM APIとの通信を担当し、効率的かつ信頼性の高い接続を維持します。

**主な責務**:
- リクエスト送信と応答受信
- レート制限の管理
- エラーハンドリングとリトライ
- 接続状態の監視

### 3.5 テンプレート管理

様々なユースケースに特化したプロンプトテンプレートを管理します。

**主な責務**:
- プロンプトテンプレートの保存と取得
- テンプレート内の変数置換
- プロジェクト固有テンプレートの管理
- テンプレートのバージョン管理

### 3.6 レスポンスプロセッサー

LLMからのレスポンスを処理し、適切な形式でアプリケーション層に返します。

**主な責務**:
- ストリーミングレスポンスの処理
- レスポンスの解析と構造化
- 応答品質の評価
- コンテンツのフィルタリングと整形

## 4. 主要機能

### 4.1 質問応答機能

ユーザーの質問に対して、文書コンテキストを参照しながら回答を生成します。

**機能詳細**:
- 質問の意図理解と解釈
- 関連するドキュメントセクションの特定
- 回答生成のためのコンテキスト構築
- 根拠に基づく回答の提供
- ソース引用の明示

**プロンプト例**:
```
あなたは学術文書の理解を支援するAIアシスタントです。
以下の文書内容に基づいて質問に回答してください。
文書の内容に明示されていない情報については「文書に記載がありません」と回答してください。

---- 文書コンテキスト ----
{context}
------------------------

質問: {question}
```

### 4.2 概念説明機能

専門的な概念や用語についての詳細な説明を生成します。

**機能詳細**:
- 専門用語の詳細解説
- 複数の視点からの説明
- 様々な難易度レベルでの解説（初級者向け、上級者向けなど）
- 具体例や類似概念の提示
- 視覚的表現（数式、図表の説明）

**プロンプト例**:
```
以下の専門概念について、複数の難易度レベルで説明してください。
文書コンテキストに基づいて説明し、必要に応じて関連概念や例も提示してください。

概念: {concept}

---- 文書コンテキスト ----
{context}
------------------------

1. 初学者向け説明（高校生レベル）:
2. 大学生向け説明:
3. 専門家向け説明:
4. 関連する概念:
5. 具体例:
```

### 4.3 要約生成機能

文書や章・節の内容を様々な粒度で要約します。

**機能詳細**:
- 複数の長さによる要約（短文、中文、長文）
- 章・節・項などの階層別要約
- 重要ポイントのリスト化
- キーワードとキーコンセプトの抽出
- 前提知識の特定

**プロンプト例**:
```
以下の学術文書セクションの内容を要約してください。
複数の長さで要約を生成し、重要なポイントをリスト化してください。

---- 対象セクション ----
{section_content}
------------------------

1. 1文要約 (25単語以内):
2. 段落要約 (100単語程度):
3. 詳細要約 (300単語程度):
4. 重要ポイント (箇条書き5-7項目):
5. キーコンセプト (3-5項目):
```

### 4.4 自動注釈生成機能

文書内の専門用語や複雑な概念に対する注釈を自動生成します。

**機能詳細**:
- 用語の定義と背景情報の提供
- 数式の説明と各変数の意味
- 参考文献の関連付け
- 関連する図表への参照
- 追加学習リソースの提案

**プロンプト例**:
```
以下の学術文書から抽出された用語または数式に対する注釈を生成してください。
簡潔で明確な説明を提供し、必要に応じて関連する参考情報も含めてください。

---- 文書コンテキスト ----
{surrounding_context}
------------------------

注釈対象: {term_or_equation}

注釈形式:
1. 基本定義:
2. 詳細説明:
3. 関連概念:
4. 参考情報:
```

### 4.5 学習ガイド生成機能

特定のトピックや概念に関する学習ガイドを生成します。

**機能詳細**:
- 段階的な学習パスの提案
- 理解度確認のための質問生成
- 関連トピックの紹介
- 学習リソースの推奨
- 練習問題の提案

**プロンプト例**:
```
以下の学術トピックについて、効果的な学習ガイドを作成してください。
段階的な学習パスと理解度を確認するための質問を含めてください。

トピック: {topic}

---- 関連文書コンテキスト ----
{context}
--------------------------------

学習ガイド形式:
1. 前提知識:
2. 学習目標:
3. 段階的学習パス:
   a. 初級レベル
   b. 中級レベル
   c. 上級レベル
4. 理解度確認質問 (各レベル2-3問):
5. 関連トピック:
6. 推奨リソース:
```

## 5. コンテキスト管理戦略

### 5.1 コンテキスト選択アルゴリズム

質問や要求に最も関連性の高いコンテキストを選択する戦略です。

**実装方法**:
1. **キーワードベースのマッチング**:
   - 質問から抽出したキーワードと文書セクションのマッチング
   - TF-IDF等の重み付けによる関連度計算

2. **セマンティック検索**:
   - 質問と文書セクションの埋め込みベクトル生成
   - コサイン類似度に基づく関連セクションのランキング

3. **ハイブリッドアプローチ**:
   - キーワードマッチングとセマンティック検索の組み合わせ
   - 文書の階層構造を考慮した重み付け

4. **履歴を考慮した検索**:
   - 会話の流れと前後の質問を考慮したコンテキスト選択
   - ユーザーの探索パターンの学習

### 5.2 長文コンテキストの管理

LLMのコンテキストウィンドウ制限内で最大限の情報を提供する戦略です。

**実装方法**:
1. **チャンク分割**:
   - 意味のある単位（段落、セクション）でのチャンク化
   - 重複するコンテンツに対するスライディングウィンドウアプローチ

2. **優先順位付け**:
   - 関連度に基づくコンテキストの優先順位付け
   - 最も関連性の高いチャンクを優先的に含める

3. **要約と圧縮**:
   - 長いセクションの自動要約によるコンテキスト圧縮
   - 冗長な情報の削減

4. **文書構造の活用**:
   - 目次情報と階層構造の提供
   - 親セクションの概要を含めたコンテキスト構築

### 5.3 トークン管理

APIコストと応答時間を最適化するためのトークン数管理です。

**実装方法**:
1. **トークン数推定**:
   - テキストからのトークン数推定アルゴリズム
   - モデル別のトークン化方法の違いを考慮

2. **動的コンテキストサイズ調整**:
   - 質問の複雑さに基づくコンテキストサイズの調整
   - API料金対効果を考慮した最適サイズの設定

3. **キャッシュ戦略**:
   - 頻繁に利用されるコンテキストのキャッシュ
   - 一度計算したトークン数の再利用

## 6. モデル選択戦略

タスクの性質や要求に応じて最適なLLMを選択する戦略です。

### 6.1 自動モデル選択

**選択基準**:
1. **タスクの複雑さ**:
   - 単純な質問応答 → 小規模・高速モデル
   - 複雑な推論や説明 → 大規模・高性能モデル

2. **ドメイン特化要件**:
   - 数学的推論 → 数学能力の高いモデル
   - コード理解 → プログラミング能力の高いモデル

3. **レスポンス時間要件**:
   - 即時応答が必要 → 高速モデル
   - 品質優先 → 高精度モデル（処理時間が長くても許容）

4. **コスト考慮**:
   - 予算制約に基づくモデル選択
   - ユーザープランに応じた制限

### 6.2 フォールバックメカニズム

一つのモデルが失敗した場合の代替戦略です。

**実装方法**:
1. **エラー検出**:
   - API接続エラーの検出
   - レスポンス品質の評価

2. **自動切り替え**:
   - プライマリモデルが失敗した場合のセカンダリモデルへの切り替え
   - 段階的なフォールバック優先順位

3. **ハイブリッド利用**:
   - 複数モデルの並行利用と結果の比較
   - モデル間の相互確認

## 7. プロンプトエンジニアリング戦略

効果的なプロンプト設計の方針と最適化戦略です。

### 7.1 専門分野別プロンプト最適化

**戦略**:
1. **数学分野**:
   - 数式の正確な解釈を促すガイドライン
   - ステップバイステップの解法を要求する構造
   - 数学記号の適切な使用と解釈

2. **物理学分野**:
   - 物理法則の正確な適用を促す構造
   - 単位系の一貫性確保
   - 図表の解釈と視覚的説明の生成

3. **コンピュータサイエンス分野**:
   - アルゴリズムの擬似コード表現
   - 計算量と効率性の分析
   - コードの実装例と説明

### 7.2 プロンプトテンプレートの例

数学分野における問題解決支援テンプレート:

```
あなたは数学の専門家アシスタントです。
以下の数学的問題について、段階的に詳細な解法を提供してください。
各ステップの根拠を説明し、最終的な答えに至るまでの過程を明確に示してください。

問題: {mathematical_problem}

---- 関連文書コンテキスト ----
{context}
--------------------------------

解答形式:
1. 問題の解釈:
2. 適用すべき数学的概念:
3. 解法手順:
   a. ステップ1: [説明と計算]
   b. ステップ2: [説明と計算]
   ...
4. 最終解答:
5. 検証:
6. 関連する概念や一般化:
```

### 7.3 プロンプト最適化プロセス

**実装方法**:
1. **A/Bテスト**:
   - 異なるプロンプト変種のパフォーマンス比較
   - ユーザーフィードバックに基づく改善

2. **自動評価メトリクス**:
   - 回答品質の客観的評価
   - 回答生成時間の測定

3. **専門家レビュー**:
   - 分野専門家によるプロンプトと応答の検証
   - 専門的正確性の確保

4. **継続的改善**:
   - 成功事例と失敗事例の分析
   - パターン認識に基づくテンプレート最適化

## 8. API連携の実装詳細

### 8.1 API通信モジュール

**主要コンポーネント**:
1. **クライアント実装**:
   - HTTP非同期クライアント (aiohttp)
   - レート制限対応
   - タイムアウト処理

2. **認証管理**:
   - APIキーのセキュアな保存
   - ユーザー別APIキー管理
   - 認証トークンの更新処理

3. **リクエスト構築**:
   - 各APIに特化したリクエスト形式の構築
   - パラメータのバリデーション
   - リクエストのログ記録

### 8.2 実装例: Claude API連携

```python
# Claude API連携の実装例
class ClaudeAPIClient:
    def __init__(self, api_key: str, base_url: str = "https://api.anthropic.com"):
        self.api_key = api_key
        self.base_url = base_url
        self.session = None
    
    async def _ensure_session(self):
        if self.session is None:
            self.session = aiohttp.ClientSession(
                headers={
                    "x-api-key": self.api_key,
                    "anthropic-version": "2023-06-01"
                }
            )
    
    async def close(self):
        if self.session:
            await self.session.close()
            self.session = None
    
    async def complete(self, 
                      prompt: str, 
                      model: str = "claude-3-opus-20240229", 
                      max_tokens: int = 1000,
                      temperature: float = 0.7,
                      stream: bool = False):
        """Claude APIにリクエストを送信"""
        await self._ensure_session()
        
        payload = {
            "model": model,
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": max_tokens,
            "temperature": temperature,
            "stream": stream
        }
        
        try:
            if stream:
                return await self._stream_response(payload)
            else:
                return await self._complete_response(payload)
        except Exception as e:
            logger.error(f"Claude API error: {str(e)}")
            raise APIConnectionError(f"Failed to connect to Claude API: {str(e)}")
    
    async def _complete_response(self, payload: dict):
        """通常の応答を処理"""
        url = f"{self.base_url}/v1/messages"
        async with self.session.post(url, json=payload) as response:
            if response.status != 200:
                error_text = await response.text()
                raise APIResponseError(f"Claude API error: {response.status}, {error_text}")
            
            result = await response.json()
            return result
    
    async def _stream_response(self, payload: dict):
        """ストリーミング応答を処理"""
        url = f"{self.base_url}/v1/messages"
        async with self.session.post(url, json=payload) as response:
            if response.status != 200:
                error_text = await response.text()
                raise APIResponseError(f"Claude API error: {response.status}, {error_text}")
            
            async for line in response.content:
                if line:
                    line = line.decode('utf-8').strip()
                    if line.startswith('data: ') and not line.startswith('data: [DONE]'):
                        chunk = json.loads(line[6:])
                        yield chunk
```

## 9. ストリーミングレスポンス処理

### 9.1 ストリーミング実装

**主要機能**:
1. **サーバー側処理**:
   - LLM APIからのイベントストリーム受信
   - チャンク処理とパース
   - 進捗モニタリング

2. **クライアント送信**:
   - Server-Sent Events (SSE)
   - WebSocketによるリアルタイム配信
   - バッファリングと安定化

3. **エラー処理**:
   - 接続断絶時の回復戦略
   - 部分応答の蓄積と再送

### 9.2 FastAPI実装例

```python
# FastAPIでのストリーミング応答実装例
@router.post("/stream_answer", response_class=StreamingResponse)
async def stream_answer(request_data: QuestionRequest):
    llm_service = get_llm_service()
    
    async def event_generator():
        try:
            async for chunk in llm_service.generate_streaming_answer(
                question=request_data.question,
                context_id=request_data.context_id,
                model=request_data.model
            ):
                if chunk:
                    yield f"data: {json.dumps(chunk)}\n\n"
        except Exception as e:
            logger.error(f"Streaming error: {str(e)}")
            yield f"data: {json.dumps({'error': str(e)})}\n\n"
        finally:
            yield "data: [DONE]\n\n"
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            'Cache-Control': 'no-cache',
            'Connection': 'keep-alive',
            'X-Accel-Buffering': 'no'
        }
    )
```

## 10. モニタリングと品質管理

### 10.1 モニタリングメトリクス

**主要メトリクス**:
1. **パフォーマンス指標**:
   - API応答時間
   - トークン処理速度
   - ストリーミング速度

2. **品質指標**:
   - 応答の正確性評価
   - ソース引用率
   - ユーザーフィードバックスコア

3. **コスト指標**:
   - トークン使用量
   - API呼び出しコスト
   - モデル別効率性

### 10.2 品質向上の仕組み

**実装方法**:
1. **フィードバックループ**:
   - ユーザーフィードバックの収集
   - 応答品質と関連度の評価
   - プロンプト改善への反映

2. **継続的な評価**:
   - 定期的なベンチマークテスト
   - 専門分野別の正確性評価
   - ユーザー満足度調査

3. **コンテンツモデレーション**:
   - 不適切なコンテンツのフィルタリング
   - バイアスとファクトチェック
   - 専門的正確性の確保

## 11. 拡張性と今後の方針

### 11.1 新規モデル対応

**方針**:
1. **アダプター拡張**:
   - 新規LLMプロバイダー用アダプターの追加
   - 統一インターフェースの維持
   - モデル固有機能の活用

2. **モデル評価フレームワーク**:
   - 新モデルのベンチマーク評価
   - 専門分野別パフォーマンス測定
   - コスト効率分析

### 11.2 機能拡張予定

**今後の計画**:
1. **マルチモーダル対応**:
   - 図表認識と生成機能
   - 数式と図表の相互変換
   - ビジュアル説明生成

2. **対話型学習アシスタント**:
   - パーソナライズされた学習計画
   - 理解度に基づく適応型質問
   - 学習進捗の追跡と最適化

3. **高度な推論支援**:
   - チェーン・オブ・ソート推論の実装
   - 複雑な問題の分解と構造化
   - エキスパート知識の統合
